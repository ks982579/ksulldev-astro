---
layout: '@layouts/NotesLayout.astro'
title: 'Select Artificial Intelligence Techniques'
pubDate: 2023-10-16
description: 'We are now aiming our scopes at artificial intelligence with the weapon of data science.'
author: 'Kevin Sullivan'
tags: ["data science", "notes", "masters", "math", "maths"]
---

```yaml
title: Data Science
subtitle: DLMBDSA01
authors: Prof. Dr. Claudia Heß
publisher: IU International University of Applied Sciences
date: 2022
```

Not sure if this belongs here or in its own Artificial Intelligence section, but here is good for now. 

# Unit 6: Selected Artificial Intelligence Techniques

pp. 119 - 141

Learning objectives include:
+ Data classification by support vector machines.
+ The _feedforward_ neural network structure.
+ The back propagation algorithm in neural networks.
+ How to develop an artificial neural networks prediction model.
+ Recurrent networks and reinforcement learning.
+ Basics about Genetic Algorithms, Fuzzy Logic, and Naïve Bayes classification. 

## Introduction

Regression is the go-to supervised learning data science model. Then there's classification, where a prediction model is developed based on the input dataset in order to estimate the class of a new data record. 

This unit covers things from support vector machines and artificial neural networks, to fuzzy logic and genetic algorithms. A little bit of a shoutout, but "Intelligent Techniques for Data Science", by R. Akerar and P.S. Sajja covers most of these topics as well and provides a high-level overview of concepts and not diving too deeply in the math and implementation. 

## 6.1 - Support Vector Machines

**Definition - Support Vector Machines (SVM):** A supervised learning algorithm in machine learning that is typically used in classification problems. Can also be used in regression. 

Support Vector Machines is a binary linear classification technique where classification rule is to develop a linear function of the input dataset variables $\{x_{1,k}, x_{2,k},\dots,x_{M,k}\}$, with $k=1,2,\dots,N$, and $N$ and $M$ are the number of data records and data variables respectively. 

the classification function can be formulated as the linear equation:

$$
w \cdot x_i+b=0
$$

For an example, and I thought about this, we have a large group of people. We give them a test on a computer. If they pass the test, we consider them computer literate. If they fail, we don't. 

Our classification line will separate the dataset according to class, with one class on each side of it. 

There's also now a **separating channel**, which is the channel that has the classification line on its center line and is bounded by support vectors on both sides. It is generated by the classification line and defined by two lines parallel to it and equidestant from both sides, namely:

$$
\begin{align*}
+1 &= w \cdot x_i +b\\\\
\text{and}\\\\
-1 &= w \cdot x_i +b
\end{align*}
$$

Distance between support vector lines, or the width of the channel, is called the **margin**. **Support vectors** are data records $x_i$ that lie on either side of the separating channel. The margin is apparently denoted as $l$, a lower-case $L$. I'll denote below with var-phi $(\varphi)$ to avoid mistaking with number 1 or something:

$$
\begin{align*}
l = \varphi &= x_i^{+1} - x_i^{-1}\\
\therefore \varphi &= \frac{+1-b}{w}- \frac{-1-b}{w} = \frac{2}{\|w\|}
\end{align*}
$$

The SVM technique seeks the maximum margin to obtain the optimum separation between 2 classes. We seek classification equation that yields maximum $\frac{2}{\|w\|}$. 

### Kernel Trick

The _Kernel Trick_ helps SVM handle nonlinearly separable datasets. The dataset is projected onto a higher dimensional space unto which the dataset is linearly separable. The course book has a cool figure where a 2D dataset is projected into 3D and a linear plane can separate classes where a linear line could not. 

The SVM merely considers relative distance between data points in a _virtual_ projection space given by an appropriate Kernel function. Actual projection is apparently unnecessary. 

Common Kernel functions include:
+ polynomial
+ sigmoid
+ radial

There's a library for support vector machines called LIBSVM. Here's a pdf called [A Library for Support Vector Machines](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf), and the library is at [www.csie.ntu.edu.tw](http://www.csie.ntu.edu.tw/~cjlin/libsvm). Should work with Python:

```bash
pip install -U libsvm-official
```

Additionally, [SciKit-Learn](https://scikit-learn.org/stable/modules/svm.html) apparently also has resources. 

## 6.2 - Artificial Neural Networks

p. 122

The purpose of developing an _artificial neural network_ (ANN) is to produce an artificial system capable of performing sophisticated calculations similar to the human brain. 

**Definition - Artificial Neural Network:** A computing network based on the neural networks of animal brains. It contains nodes and arrows. 

The response is encoded in how and to what degree various neurons are connected. The basic architecture comprises many layers of neurons:
+ Input layer for input values of the dataset variables.
+ Output layer for producing the value of the target variable. 
+ Intermediate layer (AKA hidden layers) to build more complex paths. 

**Deep learning** is the application of artificial neural networks to learning tasks with cascading hidden layers. The strength of a link between two neurons ($j$ and $i$) on two adjacent layers is represented by a weight values $w_{ji}$. The network adjusts weights of its links to produce output value close to the desired value of the target variable.

Somehow, the neuron sums its weighted inputs coming from its preceding links to get $a_i$ and then applies a transfer function to produce output $z_i$. 

$$
\begin{gather*}
a_i = \sum_{j=1}^M (w_{ji}\cdot z_j)\\
z_i = f(a_i)
\end{gather*}
$$

Note that the $z_j$ comes from the neurons before it, and $z_i$ is passed on to the next neuron. 

The _transfer function_ $(f)$ is also called an _activation function_. It must be continuous, differentiable, non-decreasing, and easy to compute. The neurons have mostly nonlinear activation functions which allow the network to learn nonlinear and linear relationships between the variables. 

I'll include the list of common activation functions because it's new to me:

**Linear (lin):** function generates outputs which are not confined to a specific range.

$$
z=a
$$

The graph is more like a $y=mx+b$ thing, but ok.

**log-sigmoid (logsig):** function generates outputs between 0 and 1

$$
z = \frac{1}{1+e^{-a}}
$$

That equations has an actuarial look to it. 

**Tan-sigmoid (tansig):** Function generates output between $-1 \le z \le +1$

$$
z = \tanh(a) = \frac{e^{2a}-1}{e^{2a}+1}
$$

**Exponential Linear Unit (ELU):** Function generates outputs that are not confined to a specific range. It has a linear shape for positive inputs and an exponential shape for negative outputs. 

$$
z = \left\{ 
\begin{array}{ll}
a & a \gt 0\\
\alpha (e^a-1) & a \le 0
\end{array}
\right.
$$

Alpha $(\alpha)$ is some positive values usually equal to $0.01$. 

**Rectified Linear Unit (ReLU):** Function generates outputs that are 0 for inputs with negative values and for all other inputs, the output will equal the input number.

$$
z = \max(a,\ 0)
$$

### Feedforward Networks

[Feedforward neural network | Wikipedia](https://en.wikipedia.org/wiki/Feedforward_neural_network): is on of the broad types of ANN characterized by the direction of the flow of information between its layers. The flow is _uni-directional_, meaning it only flows in one direction, which is forward, from input nodes to output nodes through hidden nodes (if any). Contrast to _recurrent neural network_, which have an bi-directional flow and we will look at later. Modern feedforward networks are trained using _backpropagation_ method, coming up soon, and are colloquially referred to as the "vanilla" neural networks. 

The Wikipedia article goes deeper into activation functions (both sigmoids), and then covers a brief history of the [perceptron | Wikipedia](https://en.wikipedia.org/wiki/Perceptron). 

[Feedforward Neural Networks | Brilliant](https://brilliant.org/wiki/feedforward-neural-networks/): These are ANNs where connections between units do not form a cycle. These were  first type of ANN invented and are simpler than their counterpart, the _recurrent neural network_. Information only travels forward in the network, there are no loops, first through input nodes, then through hidden nodes (if any), and finally through the output nodes. Primarily used for supervised learning where data to be learned is neither sequential nor time-dependent. 

[Feed Forward Neural Network | DeepAI](https://deepai.org/machine-learning-glossary-and-terms/feed-forward-neural-network): Same as other definitions, information flows forward and it is the simplest form of neural network. 

The number of neurons, number of hidden layers, and neurons' activation functions are completely arbitrary. Some feedforward network rules:
1. No connections within the neurons of a layer.
2. No direct connections between input layer and output layer. I guess that means at least one hidden layer.
3. Fully connected between layers. That is, each neuron in one layer is connected to all neurons in its succeeding layer.
4. Can have a number of hidden neurons per layer that is more or less than the number of inputs. 

So we specify the following:
+ Number of hidden layers.
+ Number of neurons within each hidden layer.
+ Activation function of each neuron.

And our network is ready to use a given dataset to self-learn. Per usual data science stuff, divide the dataset into training and testing, optionally a validation as well. The book suggest training at about 60% and testing around 40%, which seems like too much testing for me. 

A common learning algorithm is the back propagation algorithm. 

### Back Propagation Algorithm

p. 127
