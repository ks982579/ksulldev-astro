---
layout: '@layouts/NotesLayout.astro'
title: 'Select Artificial Intelligence Techniques'
pubDate: 2023-10-16
description: 'We are now aiming our scopes at artificial intelligence with the weapon of data science.'
author: 'Kevin Sullivan'
tags: ["data science", "notes", "masters", "math", "maths"]
---

```yaml
title: Data Science
subtitle: DLMBDSA01
authors: Prof. Dr. Claudia Heß
publisher: IU International University of Applied Sciences
date: 2022
```

Not sure if this belongs here or in its own Artificial Intelligence section, but here is good for now. 

# Unit 6: Selected Artificial Intelligence Techniques

pp. 119 - 141

Learning objectives include:
+ Data classification by support vector machines.
+ The _feedforward_ neural network structure.
+ The back propagation algorithm in neural networks.
+ How to develop an artificial neural networks prediction model.
+ Recurrent networks and reinforcement learning.
+ Basics about Genetic Algorithms, Fuzzy Logic, and Naïve Bayes classification. 

## Introduction

Regression is the go-to supervised learning data science model. Then there's classification, where a prediction model is developed based on the input dataset in order to estimate the class of a new data record. 

This unit covers things from support vector machines and artificial neural networks, to fuzzy logic and genetic algorithms. A little bit of a shoutout, but "Intelligent Techniques for Data Science", by R. Akerar and P.S. Sajja covers most of these topics as well and provides a high-level overview of concepts and not diving too deeply in the math and implementation. 

## 6.1 - Support Vector Machines

**Definition - Support Vector Machines (SVM):** A supervised learning algorithm in machine learning that is typically used in classification problems. Can also be used in regression. 

Support Vector Machines is a binary linear classification technique where classification rule is to develop a linear function of the input dataset variables $\{x_{1,k}, x_{2,k},\dots,x_{M,k}\}$, with $k=1,2,\dots,N$, and $N$ and $M$ are the number of data records and data variables respectively. 

the classification function can be formulated as the linear equation:

$$
w \cdot x_i+b=0
$$

For an example, and I thought about this, we have a large group of people. We give them a test on a computer. If they pass the test, we consider them computer literate. If they fail, we don't. 

Our classification line will separate the dataset according to class, with one class on each side of it. 

There's also now a **separating channel**, which is the channel that has the classification line on its center line and is bounded by support vectors on both sides. It is generated by the classification line and defined by two lines parallel to it and equidestant from both sides, namely:

$$
\begin{align*}
+1 &= w \cdot x_i +b\\\\
\text{and}\\\\
-1 &= w \cdot x_i +b
\end{align*}
$$

Distance between support vector lines, or the width of the channel, is called the **margin**. **Support vectors** are data records $x_i$ that lie on either side of the separating channel. The margin is apparently denoted as $l$, a lower-case $L$. I'll denote below with var-phi $(\varphi)$ to avoid mistaking with number 1 or something:

$$
\begin{align*}
l = \varphi &= x_i^{+1} - x_i^{-1}\\
\therefore \varphi &= \frac{+1-b}{w}- \frac{-1-b}{w} = \frac{2}{\|w\|}
\end{align*}
$$

The SVM technique seeks the maximum margin to obtain the optimum separation between 2 classes. We seek classification equation that yields maximum $\frac{2}{\|w\|}$. 

### Kernel Trick

The _Kernel Trick_ helps SVM handle nonlinearly separable datasets. The dataset is projected onto a higher dimensional space unto which the dataset is linearly separable. The course book has a cool figure where a 2D dataset is projected into 3D and a linear plane can separate classes where a linear line could not. 

The SVM merely considers relative distance between data points in a _virtual_ projection space given by an appropriate Kernel function. Actual projection is apparently unnecessary. 

Common Kernel functions include:
+ polynomial
+ sigmoid
+ radial

There's a library for support vector machines called LIBSVM. Here's a pdf called [A Library for Support Vector Machines](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf), and the library is at [www.csie.ntu.edu.tw](http://www.csie.ntu.edu.tw/~cjlin/libsvm). Should work with Python:

```bash
pip install -U libsvm-official
```

Additionally, [SciKit-Learn](https://scikit-learn.org/stable/modules/svm.html) apparently also has resources. 

## 6.2 - Artificial Neural Networks

p. 122
